import type { LocaleStrings } from './types';

export const nl: LocaleStrings = {
  errors: {
    appRootNotFound: 'App-root niet gevonden',
    missingUiElement: 'Vereist UI-element ontbreekt',
    diagramRenderFailed: 'Diagram kon niet worden getekend. Probeer te vernieuwen.',
  },
  defaultDataset: ['anna', 'bob', 'carla', 'diana', 'elias', 'frank', 'lucas', 'mila', 'nora'].join('\n'),
  flowStages: [
    { id: 'dataset', title: '1. Dataset', description: 'Lees namen in en splits in train/dev/test.' },
    { id: 'encode', title: '2. Codering', description: 'Zet tekens om naar gehele token-ID’s (BOS = begin‑van‑sequentie).' },
    { id: 'context', title: '3. Contextvenster', description: 'Vast contextvenster (block_size); elke positie voorspelt de volgende token.' },
    { id: 'forward', title: '4. Voorwaartse stap', description: 'Token + positiebedding → RMSNorm → Attention (Q,K,V, multi-head) → residu → RMSNorm → MLP (ReLU) → residu → lm_head → logits.' },
    { id: 'softmax', title: '5. Softmax', description: 'Zet logits om in waarschijnlijkheden voor volgende tekens.' },
    { id: 'loss', title: '6. Verlies', description: 'Cross‑entropy vergelijkt de voorspelde verdeling met de doeltoken.' },
    { id: 'backprop', title: '7. Backprop', description: 'Autograd berekent de gradiënt voor elke parameter.' },
    { id: 'update', title: '8. Update', description: 'Adam: m = β1·m + (1−β1)·g, v = β2·v + (1−β2)·g²; parameter -= lr·m_hat/(√v_hat + ε).' },
  ],
  illustrationLabels: {
    dataset: { train: 'train', dev: 'dev', test: 'test', names: 'namen', namesPerLine: '1 per regel' },
    encode: { charA: 'a', charN: 'n', id0: '0', id1: '1' },
    context: { next: 'volgende?', pos0: 'pos 0', block: 'blok' },
    forward: { embed: '+ embed', attn: 'Attn', mlp: 'MLP', logits: 'logits' },
    softmax: { logits: 'logits', raw: '(ruw)', expSum: 'exp / Σ', probsSum1: 'kansen Σ = 1' },
    loss: { pTarget: 'p(doel)', probability: 'kans', negLog: '−log(·)', L: 'L', loss: 'verlies' },
    backprop: { params: 'parameters', gradLabel: '∂L∕∂', L: 'L', backward: 'backward' },
    update: { param: 'parameter', adam: 'Adam', adamFormula: 'm,v ← grad · parameter −= lr·m̂/√v̂', updated: 'bijgewerkt' },
  },
  explainerBodies: {
    dataset: `We beginnen met een lijst namen (één per regel). Die lijst wordt opgesplitst in drie delen: <strong>train</strong>, <strong>dev</strong> en <strong>test</strong>. Het model leert alleen van de train‑set. De dev‑set vertelt ons hoe goed het gaat tijdens het trainen (bijv. de “dev loss” die je ziet). De test‑set bewaren we tot het eind om de uiteindelijke prestatie te meten. Typisch gebruiken we ~80% voor train, 10% voor dev en 10% voor test.`,
    encode: `Het model kan niet rechtstreeks met letters werken; het heeft getallen nodig. Elk teken (zoals <code>a</code>, <code>n</code>) wordt daarom omgezet in een getal, een <strong>token‑ID</strong>. Een speciaal token markeert het begin van een naam (<strong>BOS</strong> = begin van de sequentie). Later zet een “embedding”-laag deze ID’s om in vectoren met getallen die het model daadwerkelijk gebruikt. Denk aan: letters → ID‑nummers → rijke numerieke vectoren.<br/><br/><strong>Wat is een token in een LLM‑context?</strong> Een <strong>token</strong> is het kleinste tekstdeeltje waarmee het model werkt: hier één teken; in grotere modellen vaak een subwoord of woord. De <strong>context</strong> is de lijst tokens die we het model in één keer geven (bijv. de vorige paar tekens). Het model leest die context en voorspelt de volgende token. Dus “context” = de invoersequentie; “token” = één element in die sequentie (of de volgende die we voorspellen).`,
    context: `Het model ziet niet de hele naam in één keer. Het kijkt alleen naar een vast aantal vorige tekens; dat is het <strong>contextvenster</strong> (of blok). Op elke stap probeert het het <strong>volgende</strong> teken te raden. Bijvoorbeeld: als de context <code>a, n, n</code> is, kan het doel <code>a</code> zijn (voor “anna”). Daarna schuiven we één teken op en herhalen we dat. Zo levert één naam veel kleine “voorspel het volgende teken”-taken op.`,
    forward: `Dit is één volledige voorwaartse stap door het netwerk. We nemen de token‑ID’s en hun posities, zetten die om in vectoren (embeddings) en tellen ze op. Daarna gaat het door: een normalisatiestap (<strong>RMSNorm</strong>), <strong>attention</strong> (zodat elke positie naar de andere kan kijken), een kleine “MLP”-blok en tot slot een kop die per mogelijk volgend teken één score uitstuurt; dat zijn de <strong>logits</strong>. Hier vindt nog geen leren plaats; we berekenen alleen de huidige voorspelling van het model.`,
    softmax: `Het netwerk produceert ruwe scores (logits). We hebben kansen nodig: “hoe waarschijnlijk is elk teken als volgende?”. <strong>Softmax</strong> doet dat. Het zet logits om in getallen tussen 0 en 1 die samen precies 1 zijn (een kansverdeling). De formule: elke kans = exp(logit) gedeeld door de som van exp over alle logits. Tijdens training proberen we de kans op het juiste volgende teken zo hoog mogelijk te maken.`,
    loss: `We willen één getal dat zegt “hoe fout was de voorspelling?”. Dat is de <strong>loss</strong>. Hier gebruiken we <strong>cross‑entropy</strong>: <code>L = -log(kans die we aan het juiste teken gaven)</code>. Was het model zelfverzekerd én correct, dan is die kans hoog en de loss laag. Was het fout of onzeker, dan is de loss hoger. Training probeert deze loss in de tijd te verlagen.`,
    backprop: `Als we de loss hebben, moeten we weten hoe we elke gewicht in het netwerk moeten aanpassen om die loss te verlagen. <strong>Backpropagation</strong> doet dat: het loopt vanaf de loss achteruit door alle lagen (attention, MLP, embeddings) en berekent een <strong>gradiënt</strong> voor elke parameter. De gradiënt geeft de richting en ongeveer de grootte van de aanpassing aan. De “gradient norm” die je ziet is één getal dat samenvat hoe groot die gradiënten gemiddeld zijn.<br/><br/><strong>Wat betekent ∂L∕∂?</strong> Het symbool <strong>∂L∕∂</strong> (lees: "partiële d L over partiële d …") is de calculusnotatie voor <em>hoeveel de loss L verandert als je één parameter verandert</em>. Voor elk gewicht krijgen we een getal: <code>∂L∕∂(gewicht)</code>. Is dat getal positief, dan zou het verhogen van het gewicht de loss verhogen (dus verlagen we het gewicht). Is het negatief, dan verhogen we het gewicht. ∂L∕∂ is dus precies de gradiënt: het zegt per parameter welke kant we op moeten om de loss te verlagen.`,
    update: `We hebben nu gradiënten; nu passen we de gewichten echt aan. We gebruiken <strong>Adam</strong>, een populaire optimizer. Die houdt per parameter een soort “geheugen” (momentum) en “spreiding” (variantie) bij en werkt elk gewicht bij met de learning rate en die termen: <code>param -= lr · m_hat / (√v_hat + ε)</code>. De learning rate wordt vaak gedurende de training kleiner (bijv. lineaire afbouw), zodat de stappen naar het einde toe kleiner worden.`,
  },
  aria: {
    close: 'Sluiten',
    learnMoreAbout: 'Meer over',
    explainTrainingDynamics: 'Leg de trainingsdynamiekgrafiek uit',
  },
  header: {
    panelTitle: 'microgpt in de browser',
    title: 'Visualiseer het volledige algoritme, live',
    subtitle: 'TypeScript-port van <a href="https://karpathy.github.io/2026/02/12/microgpt/" target="_blank" rel="noopener noreferrer" class="underline text-neon/90 hover:text-neon focus:outline focus:ring-2 focus:ring-neon/50 rounded">microgpt</a> door <a href="https://karpathy.github.io/" target="_blank" rel="noopener noreferrer" class="underline text-neon/90 hover:text-neon focus:outline focus:ring-2 focus:ring-neon/50 rounded">Andrej Karpathy</a>.',
    description: 'Dit dashboard legt elke fase van microGPT uit en toont tijdens het trainen actuele waarden: contexttokens, kansen, verlies, gradiëntnorm en parameterupdates.',
  },
  generatedName: {
    panelTitle: 'Gegenereerde naam',
    generate: 'Genereer',
  },
  controls: {
    panelTitle: 'Besturing',
    datasetLabel: 'Dataset (1 naam per regel)',
    maxSteps: 'Max stappen',
    evalEvery: 'Eval elke',
    start: 'Start',
    pause: 'Pauze',
    reset: 'Reset',
    tipReset: 'Tip: reset na het wijzigen van dataset of hyperparameters.',
  },
  trainingDynamics: {
    panelTitle: 'Trainingsdynamiek',
    explainBtn: 'Leg de trainingsdynamiekgrafiek uit',
    statusIdle: 'inactief',
    statusTraining: 'trainen',
    statusCompleted: 'klaar',
    step: 'Stap',
    batchLoss: 'Batch‑verlies',
    trainLoss: 'Train‑verlies',
    devLoss: 'Dev‑verlies',
  },
  algorithm: {
    panelTitle: 'Hoe het algoritme werkt',
    review: 'Terugkijken:',
    live: 'Live',
    viewTransformerDiagram: 'Leg de transformerstructuur en flow uit',
  },
  breakdown: {
    panelTitle: 'Huidige stap in detail',
    stepBreakdown: 'Stap {n} in detail',
    contextText: 'Contexttekst:',
    noContextText: '(nog geen echte context, alleen het interne starttoken)',
    contextIds: 'Context‑ID’s:',
    contextTokens: 'Contexttokens:',
    contextTokensBosHint: '(alleen tekens uit je data; interne markeringen zijn verborgen)',
    target: 'Doel:',
    predicted: 'Voorspeld:',
    learningRate: 'Learning rate:',
    gradientNorm: 'Gradiëntnorm:',
  },
  probabilities: {
    panelTitle: 'Kansen (huidige stap)',
    explainer:
      'Elke rij is een mogelijk volgend token. De balklengte en het getal (0–1) geven de voorspelde kans van het model voor dat token.',
  },
  languages: {
    label: 'Taal',
    en: 'Engels',
    nl: 'Nederlands',
  },
  iterationSelect: {
    stepLabel: 'Stap {n}',
    trainDevLabel: 'train={trainLoss} dev={devLoss}',
  },
  chart: {
    min: 'min',
    max: 'max',
  },
  vectorBars: {
    first8Dims: '(eerste 8 dimensies)',
  },
  stageVisual: {
    dataset: {
      splitDescription: 'Verdeling van documenten (namen) over train/dev/test.',
      train: 'train',
      dev: 'dev',
      test: 'test',
    },
    encode: {
      description: 'Tekens worden tokens (ID’s) vóór het trainen.',
      contextTokensToIds: 'contexttokens -> ID’s',
      target: 'doel:',
    },
    context: {
      description: 'Schuivend contextvenster dat de volgende token voorspelt.',
    },
    forward: {
      attentionHead0: 'Attention (head 0) over contextposities',
      tokenEmbedding: 'token‑embedding',
      positionEmbedding: 'positie‑embedding',
      sumEmbedding: 'som van embeddings',
      preHeadAfterMlp: 'pre‑head (na RMSNorm + MLP‑blok)',
      preHeadBeforeLmHead: 'pre‑head (vóór lm_head)',
    },
    softmax: {
      description: 'Zet logits om in genormaliseerde kansen.',
    },
    loss: {
      description: 'Cross‑entropy voor de echte volgende token.',
      predictedTarget: 'Voorspeld {pred}, doel {target}',
    },
    backprop: {
      description: 'Backward‑stap berekent gradiënten door de hele graaf.',
      gradientNorm: 'gradiëntnorm',
    },
    update: {
      description: 'Adam: m = β1·m + (1−β1)·g, v = β2·v + (1−β2)·g²; parameter -= lr·m_hat/(√v_hat + ε).',
      formula: 'param -= lr * m_hat / (sqrt(v_hat) + ε)',
      lrStepMagnitude: 'lr={lr} | gemiddelde stapgrootte≈{delta}',
    },
  },
  mermaid: {
    whichCharacter: 'Welk teken?',
    whichPosition: 'Welke positie?',
    turnCharIntoVector: 'Zet teken om in vector',
    addPositionAsVector: 'Voeg positie toe als vector',
    combineBoth: 'Combineer beide',
    stabilizeScale: 'Stabiliseer schaal',
    transformerBlock: 'Transformerblok × {n}',
    stabilize: 'Stabiliseer',
    attentionMixContext: 'Attention: meng met context',
    addShortcut: 'Voeg shortcut toe',
    smallFeedForward: 'Kleine feed‑forward',
    predictNextChar: 'Voorspel volgend teken',
    scoresForEachChar: 'Scores voor elk teken',
  },
  dialogs: {
    trainingDynamics: {
      title: 'De grafiek van de trainingsdynamiek begrijpen',
      whatGraphShows: 'Wat deze grafiek laat zien',
      whatGraphShowsBody: 'De lijn toont het <strong>batch‑verlies</strong> over trainingsstappen (tijd loopt van links naar rechts). Elk punt is de loss op één mini‑batch: hoe fout het model zat op dat kleine stukje data. De grafiek toont de laatste 300 stappen zodat je recente trends ziet.',
      spikesMean: 'Wat betekenen de pieken?',
      spikesMeanBody: 'Pieken zijn normaal. Elke stap gebruikt een andere willekeurige batch; sommige batches zijn lastiger dan andere, bijvoorbeeld met zeldzame tokens of “moeilijke” contexten. Daardoor kan de batch‑loss kort omhoog schieten. Zolang de <em>algemene trend</em> dalend is, leert het model. Grote, plotselinge pieken kunnen ook ontstaan als de learning rate hoog is of wanneer het model een lastig deel van de data tegenkomt.',
      numbersAboveGraph: 'Getallen boven de grafiek',
      numbersAboveGraphBody: '<strong>Batch‑loss</strong> is de waarde voor de huidige stap (wat je op de lijn ziet). <strong>Train‑loss</strong> en <strong>Dev‑loss</strong> zijn gemiddelden over de volledige train‑ en dev‑set en worden elke “Eval elke”-stappen bijgewerkt. Ze zijn gladder en laten zien of het model echt generaliseert (dev‑loss daalt) of overfit (train daalt, dev stijgt).',
      lowerLossNote: 'Lagere loss = betere voorspellingen. Het doel is dat de lijn daalt en dat de dev‑loss dicht bij of onder de train‑loss blijft.',
    },
    transformer: {
      title: 'Hoe het model één stap ziet',
      description: 'Een <strong class="text-neon">transformer</strong> is een neuraal netwerk dat tekst verwerkt, bijvoorbeeld de letters in een naam, door elk teken (1e, 2e, 3e, enzovoort) naar alle andere te laten &ldquo;kijken&rdquo;. Anders dan gewone feed-forward netwerken, die één teken tegelijk verwerken, of recurrent netwerken (RNN&rsquo;s), die stap voor stap werken en moeite hebben met lange tekst, zien transformers alle tekens tegelijk. Voeg <strong>attention</strong> toe, waarbij elk teken naar relevante andere kan &ldquo;kijken&rdquo;, en je krijgt een model dat goed werkt voor taal. Bij een naam als &ldquo;anna&rdquo; leest het model niet letter voor letter. Het ziet de hele string en laat elk teken de interpretatie van de rest beïnvloeden.',
      intro: 'De data stroomt <strong class="text-neon">van boven naar beneden</strong>. We beginnen met “welk teken?” en “waar in de naam?”, zetten die om in vectoren, combineren en normaliseren, en voeren dat door het transformerblok (attention + kleine MLP). Uiteindelijk krijgen we scores voor het volgende teken.',
      oneForwardPassNote: 'Dit is één “forward pass” voor één teken; dezelfde structuur herhaalt zich voor elk teken dat het model voorspelt.',
    },
  },
  samplePlaceholder: '...',
  dataStatsTemplate: 'woorden={words} | vocab={vocab} | train/dev/test={train}/{dev}/{test}',
};

