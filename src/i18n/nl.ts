import type { LocaleStrings } from './types';

export const nl: LocaleStrings = {
  errors: {
    appRootNotFound: 'App-root niet gevonden',
    missingUiElement: 'Vereist UI-element ontbreekt',
    diagramRenderFailed: 'Diagram kon niet worden getekend. Probeer te vernieuwen.',
  },
  defaultDataset: ['anna', 'bob', 'carla', 'diana', 'elias', 'frank', 'lucas', 'mila', 'nora'].join('\n'),
  flowStages: [
    { id: 'dataset', title: '1. Dataset', description: 'Lees namen in en splits in train/dev/test.' },
    { id: 'encode', title: '2. Codering', description: 'Zet tekens om naar gehele token-ID’s (BOS = begin‑van‑sequentie).' },
    { id: 'context', title: '3. Contextvenster', description: 'Vast contextvenster (block_size); elke positie voorspelt de volgende token.' },
    { id: 'forward', title: '4. Voorwaartse stap', description: 'Token + positiebedding → RMSNorm → Attention (Q,K,V, multi-head) → residu → RMSNorm → MLP (ReLU) → residu → lm_head → logits.' },
    { id: 'softmax', title: '5. Softmax', description: 'Zet logits om in waarschijnlijkheden voor volgende tekens.' },
    { id: 'loss', title: '6. Verlies', description: 'Cross‑entropy vergelijkt de voorspelde verdeling met de doeltoken.' },
    { id: 'backprop', title: '7. Backprop', description: 'Autograd berekent de gradiënt voor elke parameter.' },
    { id: 'update', title: '8. Update', description: 'Adam: m = β1·m + (1−β1)·g, v = β2·v + (1−β2)·g²; parameter -= lr·m_hat/(√v_hat + ε).' },
  ],
  illustrationLabels: {
    dataset: { train: 'train', dev: 'dev', test: 'test', names: 'namen', namesPerLine: '1 per regel' },
    encode: { charA: 'a', charN: 'n', id0: '0', id1: '1' },
    context: { next: 'volgende?', pos0: 'pos 0', block: 'blok' },
    forward: { embed: '+ embed', attn: 'Attn', mlp: 'MLP', logits: 'logits' },
    softmax: { logits: 'logits', raw: '(ruw)', expSum: 'exp / Σ', probsSum1: 'kansen Σ = 1' },
    loss: { pTarget: 'p(doel)', probability: 'kans', negLog: '−log(·)', L: 'L', loss: 'verlies' },
    backprop: { params: 'parameters', gradLabel: '∂L∕∂', L: 'L', backward: 'backward' },
    update: {
      param: 'parameter',
      grad: 'grad',
      adam: 'Adam',
      adamMomentum: 'm,v uit g',
      adamUpdate: 'param −= lr·m̂/(√v̂+ε)',
      updated: 'bijgewerkt',
    },
  },
  explainerBodies: {
    dataset: `We beginnen met een lijst namen (één per regel). Die lijst wordt opgesplitst in drie delen: <strong>train</strong>, <strong>dev</strong> en <strong>test</strong>. Het model leert alleen van de train‑set. De dev‑set vertelt ons hoe goed het gaat tijdens het trainen (bijv. de “dev loss” die je ziet). De test‑set bewaren we tot het eind om de uiteindelijke prestatie te meten. Typisch gebruiken we ~80% voor train, 10% voor dev en 10% voor test.`,
    encode: `Het model kan niet rechtstreeks met letters werken; het heeft getallen nodig. Elk teken (zoals <code>a</code>, <code>n</code>) wordt daarom omgezet in een getal, een <strong>token‑ID</strong>. Een speciaal token markeert het begin van een naam (<strong>BOS</strong> = begin van de sequentie). Later zet een “embedding”-laag deze ID’s om in vectoren met getallen die het model daadwerkelijk gebruikt. Denk aan: letters → ID‑nummers → rijke numerieke vectoren.<br/><br/><strong>Wat is een token in een LLM‑context?</strong> Een <strong>token</strong> is het kleinste tekstdeeltje waarmee het model werkt: hier één teken; in grotere modellen vaak een subwoord of woord. De <strong>context</strong> is de lijst tokens die we het model in één keer geven (bijv. de vorige paar tekens). Het model leest die context en voorspelt de volgende token. Dus “context” = de invoersequentie; “token” = één element in die sequentie (of de volgende die we voorspellen).`,
    context: `Het model ziet niet de hele naam in één keer. Het kijkt alleen naar een vast aantal vorige tekens; dat is het <strong>contextvenster</strong> (of blok). Op elke stap probeert het het <strong>volgende</strong> teken te raden. Bijvoorbeeld: als de context <code>a, n, n</code> is, kan het doel <code>a</code> zijn (voor “anna”). Daarna schuiven we één teken op en herhalen we dat. Zo levert één naam veel kleine “voorspel het volgende teken”-taken op.`,
    forward: `Dit is één volledige voorwaartse stap door het netwerk. We nemen de token‑ID’s en hun posities, zetten die om in vectoren (embeddings) en tellen ze op. Daarna gaat het door: een normalisatiestap (<strong>RMSNorm</strong>), <strong>attention</strong> (zodat elke positie naar de andere kan kijken), een kleine “MLP”-blok en tot slot een kop die per mogelijk volgend teken één score uitstuurt; dat zijn de <strong>logits</strong>. Hier vindt nog geen leren plaats; we berekenen alleen de huidige voorspelling van het model.`,
    softmax: `Het netwerk produceert ruwe scores (logits). We hebben kansen nodig: “hoe waarschijnlijk is elk teken als volgende?”. <strong>Softmax</strong> doet dat. Het zet logits om in getallen tussen 0 en 1 die samen precies 1 zijn (een kansverdeling). De formule: elke kans = exp(logit) gedeeld door de som van exp over alle logits. Tijdens training proberen we de kans op het juiste volgende teken zo hoog mogelijk te maken.`,
    loss: `We willen één getal dat zegt “hoe fout was de voorspelling?”. Dat is de <strong>loss</strong>. Hier gebruiken we <strong>cross‑entropy</strong>: <code>L = -log(kans die we aan het juiste teken gaven)</code>. Was het model zelfverzekerd én correct, dan is die kans hoog en de loss laag. Was het fout of onzeker, dan is de loss hoger. Training probeert deze loss in de tijd te verlagen.`,
    backprop: `Als we de loss hebben, moeten we weten hoe we elke gewicht in het netwerk moeten aanpassen om die loss te verlagen. <strong>Backpropagation</strong> doet dat: het loopt vanaf de loss achteruit door alle lagen (attention, MLP, embeddings) en berekent een <strong>gradiënt</strong> voor elke parameter. De gradiënt geeft de richting en ongeveer de grootte van de aanpassing aan. De “gradient norm” die je ziet is één getal dat samenvat hoe groot die gradiënten gemiddeld zijn.<br/><br/><strong>Wat betekent ∂L∕∂?</strong> Het symbool <strong>∂L∕∂</strong> (lees: "partiële d L over partiële d …") is de calculusnotatie voor <em>hoeveel de loss L verandert als je één parameter verandert</em>. Voor elk gewicht krijgen we een getal: <code>∂L∕∂(gewicht)</code>. Is dat getal positief, dan zou het verhogen van het gewicht de loss verhogen (dus verlagen we het gewicht). Is het negatief, dan verhogen we het gewicht. ∂L∕∂ is dus precies de gradiënt: het zegt per parameter welke kant we op moeten om de loss te verlagen.`,
    update: `We hebben nu gradiënten; nu passen we de gewichten echt aan. We gebruiken <strong>Adam</strong> (Adaptive Moment Estimation), een veelgebruikte optimizer die twee ideeën combineert: <strong>momentum</strong> en <strong>adaptieve leer snelheden per parameter</strong>.<br/><br/>Per parameter houdt Adam twee lopende gemiddelden bij: <strong>m</strong> (eerste moment, als momentum) vlak de gradiëntrichting af zodat we niet heen en weer slingeren; <strong>v</strong> (tweede moment) volgt het gekwadrateerde gradiënt, zodat parameters met grote gradiënten kleinere effectieve stappen krijgen. De update-regels zijn: <code>m = β₁·m + (1−β₁)·g</code> en <code>v = β₂·v + (1−β₂)·g²</code>, waarbij <em>g</em> de gradiënt is. Adam past ook <strong>bias-correctie</strong> toe (m en v delen door termen als <code>1−β₁^t</code>) omdat deze gemiddelden vroeg in de training nabij nul beginnen. De finale update is <code>param -= lr · m̂ / (√v̂ + ε)</code>, waarbij m̂ en v̂ de bias-gecorrigeerde momenten zijn. De kleine ε (bijv. 1e-8) voorkomt deling door nul.<br/><br/>Hier gebruiken we β₁=0,85 en β₂=0,99. De learning rate neemt typisch af tijdens training (bijv. lineaire decay), zodat de stappen naar het einde toe kleiner worden.`,
  },
  aria: {
    close: 'Sluiten',
    learnMoreAbout: 'Meer over',
    explainTrainingDynamics: 'Leg de trainingsdynamiekgrafiek uit',
  },
  header: {
    panelTitle: 'microgpt in de browser',
    title: 'Visualiseer het volledige algoritme, live',
    subtitle: 'TypeScript-port van <a href="https://karpathy.github.io/2026/02/12/microgpt/" target="_blank" rel="noopener noreferrer" class="underline text-neon/90 hover:text-neon focus:outline focus:ring-2 focus:ring-neon/50 rounded">microgpt</a> door <a href="https://karpathy.github.io/" target="_blank" rel="noopener noreferrer" class="underline text-neon/90 hover:text-neon focus:outline focus:ring-2 focus:ring-neon/50 rounded">Andrej Karpathy</a>.',
    description: 'Dit dashboard legt elke fase van microGPT uit en toont tijdens het trainen actuele waarden: contexttokens, kansen, verlies, gradiëntnorm en parameterupdates.',
  },
  generatedName: {
    panelTitle: 'Gegenereerde naam',
    generate: 'Genereer',
    infoBtn: 'Uitleg wat de Genereer-knop doet',
  },
  controls: {
    panelTitle: 'Besturing',
    datasetLabel: 'Dataset (1 naam per regel)',
    maxSteps: 'Max stappen',
    evalEvery: 'Eval elke',
    start: 'Start',
    pause: 'Pauze',
    reset: 'Reset',
    tipReset: 'Tip: reset na het wijzigen van dataset of hyperparameters.',
  },
  trainingDynamics: {
    panelTitle: 'Trainingsdynamiek',
    explainBtn: 'Leg de trainingsdynamiekgrafiek uit',
    statusIdle: 'inactief',
    statusTraining: 'trainen',
    statusCompleted: 'klaar',
    step: 'Stap',
    batchLoss: 'Batch‑verlies',
    trainLoss: 'Train‑verlies',
    devLoss: 'Dev‑verlies',
  },
  algorithm: {
    panelTitle: 'Hoe het algoritme werkt',
    review: 'Terugkijken:',
    live: 'Live',
    viewTransformerDiagram: 'Leg de transformerstructuur en flow uit',
  },
  breakdown: {
    panelTitle: 'Huidige stap in detail',
    stepBreakdown: 'Stap {n} in detail',
    contextText: 'Contexttekst:',
    noContextText: '(nog geen echte context, alleen het interne starttoken)',
    contextIds: 'Context‑ID’s:',
    contextTokens: 'Contexttokens:',
    contextTokensBosHint: '(alleen tekens uit je data; interne markeringen zijn verborgen)',
    target: 'Doel:',
    predicted: 'Voorspeld:',
    learningRate: 'Learning rate:',
    gradientNorm: 'Gradiëntnorm:',
  },
  probabilities: {
    panelTitle: 'Kansen (huidige stap)',
    explainer:
      'Elke rij is een mogelijk volgend token. De balklengte en het getal (0–1) geven de voorspelde kans van het model voor dat token.',
  },
  languages: {
    label: 'Taal',
    en: 'Engels',
    nl: 'Nederlands',
  },
  iterationSelect: {
    stepLabel: 'Stap {n}',
    trainDevLabel: 'train={trainLoss} dev={devLoss}',
  },
  chart: {
    min: 'min',
    max: 'max',
  },
  vectorBars: {
    first8Dims: '(eerste 8 dimensies)',
  },
  stageVisual: {
    dataset: {
      splitDescription: 'Verdeling van documenten (namen) over train/dev/test.',
      train: 'train',
      dev: 'dev',
      test: 'test',
    },
    encode: {
      description: 'Tekens worden tokens (ID’s) vóór het trainen.',
      contextTokensToIds: 'contexttokens -> ID’s',
      target: 'doel:',
    },
    context: {
      description: 'Schuivend contextvenster dat de volgende token voorspelt.',
    },
    forward: {
      attentionHead0: 'Attention (head 0) over contextposities',
      tokenEmbedding: 'token‑embedding',
      positionEmbedding: 'positie‑embedding',
      sumEmbedding: 'som van embeddings',
      preHeadAfterMlp: 'pre‑head (na RMSNorm + MLP‑blok)',
      preHeadBeforeLmHead: 'pre‑head (vóór lm_head)',
    },
    softmax: {
      description: 'Zet logits om in genormaliseerde kansen.',
    },
    loss: {
      description: 'Cross‑entropy voor de echte volgende token.',
      predictedTarget: 'Voorspeld {pred}, doel {target}',
    },
    backprop: {
      description: 'Backward‑stap berekent gradiënten door de hele graaf.',
      gradientNorm: 'gradiëntnorm',
    },
    update: {
      description: 'Adam: m = β1·m + (1−β1)·g, v = β2·v + (1−β2)·g²; parameter -= lr·m_hat/(√v_hat + ε).',
      formula: 'param -= lr * m_hat / (sqrt(v_hat) + ε)',
      lrStepMagnitude: 'lr={lr} | gemiddelde stapgrootte≈{delta}',
    },
  },
  mermaid: {
    whichCharacter: 'Welk teken?',
    whichPosition: 'Welke positie?',
    turnCharIntoVector: 'Zet teken om in vector',
    addPositionAsVector: 'Voeg positie toe als vector',
    combineBoth: 'Combineer beide',
    stabilizeScale: 'Stabiliseer schaal',
    transformerBlock: 'Transformerblok × {n}',
    stabilize: 'Stabiliseer',
    attentionMixContext: 'Attention: meng met context',
    addShortcut: 'Voeg shortcut toe',
    smallFeedForward: 'Kleine feed‑forward',
    predictNextChar: 'Voorspel volgend teken',
    scoresForEachChar: 'Scores voor elk teken',
  },
  transformerDiagramExplainers: {
    A: 'Deze stap beantwoordt: welk teken staat op deze positie in de invoer? Bijvoorbeeld, als het model voorspelt na "ann", is het teken op positie 3 misschien "a". Het model moet het ruwe invoerteken kennen om het om te zetten naar een numerieke representatie.',
    B: 'Elk teken (letter, cijfer of symbool) wordt omgezet in een vector van getallen, een embedding. Gelijksoortige tekens krijgen gelijksoortige vectoren. Deze afbeelding wordt tijdens training geleerd en laat het model met getallen werken in plaats van ruwe tekst.',
    C: 'Deze stap beantwoordt: waar in de reeks staat dit teken? Bij "anna" staat de eerste "a" op positie 0, "n" op 1, nog een "n" op 2 en de laatste "a" op 3. De positie is belangrijk omdat hetzelfde teken anders kan betekenen afhankelijk van waar het staat.',
    D: 'De positie-index wordt gecodeerd als vector (met sinus- en cosinusfuncties) en opgeteld bij de karakterembedding. Zo kent het model de volgorde: het onderscheidt de eerste "a" van de laatste "a" in "anna".',
    E: 'De karakterembedding en de positie-embedding worden opgeteld. Het resultaat is één vector die zowel het teken als zijn positie in de reeks codeert.',
    F: 'LayerNorm schaalt en verschuift de waarden zodat ze binnen gezonde grenzen blijven. Dit maakt training stabieler en helpt het model sneller te leren. Zonder dit kunnen getallen ongecontroleerd groeien of krimpen door de lagen heen.',
    TB: 'Dit is de kern van de transformer. Binnenin laat self-attention elke positie naar alle andere posities "kijken" om context op te halen. Daarna verwerkt een klein feed-forward netwerk elke positie. Het blok kan meerdere keren herhaald worden (hier één keer) om rijkere representaties op te bouwen.',
    G1: 'LayerNorm wordt toegepast vóór de attentionstap. Het normaliseert de binnenkomende vectoren zodat het attentionmechanisme waarden op een consistente schaal krijgt.',
    G2: 'Self-attention is de kerninnovatie: elke positie kan naar elke andere positie kijken. Bij het voorspellen van het volgende teken kan het model bijvoorbeeld alle vorige tekens bekijken en bepalen welke het meest relevant zijn. Dit ondersteunt afhankelijkheden over grote afstand.',
    G3: 'Een residual (of shortcut) verbinding telt de oorspronkelijke invoer weer op bij de uitvoer van attention. Dit helpt gradients tijdens training stromen en maakt leren makkelijker; het model kan de attentionlaag "overslaan" als dat nodig is.',
    G4: 'LayerNorm wordt opnieuw toegepast vóór de feed-forward stap. Elke sublaag (attention, dan feed-forward) wordt voorafgegaan door normalisatie.',
    G5: 'Een klein feed-forward netwerk: twee lineaire lagen met een niet-lineaire activatie (zoals ReLU) ertussen. Het verwerkt elke positie apart en laat het model de representaties verder transformeren.',
    G6: 'Nog een residual verbinding telt de invoer van het feed-forward blok op bij de uitvoer. Samen met G3 helpen deze shortcuts het model effectief te leren.',
    H: 'Een laatste lineaire laag mapt de getransformeerde vector naar een score voor elk teken in het vocabulaire. De uitvoer bevat één getal per mogelijk teken.',
    I: 'Deze scores geven aan hoe waarschijnlijk elk teken als volgende is. Het model kiest het teken met de hoogste score als voorspelling. In de praktijk kun je ook samplen uit een verdeling in plaats van altijd de hoogste te nemen.',
  },
  dialogs: {
    trainingDynamics: {
      title: 'De grafiek van de trainingsdynamiek begrijpen',
      whatGraphShows: 'Wat deze grafiek laat zien',
      whatGraphShowsBody: 'De lijn toont het <strong>batch‑verlies</strong> over trainingsstappen (tijd loopt van links naar rechts). Elk punt is de loss op één mini‑batch: hoe fout het model zat op dat kleine stukje data. De grafiek toont de laatste 300 stappen zodat je recente trends ziet.',
      spikesMean: 'Wat betekenen de pieken?',
      spikesMeanBody: 'Pieken zijn normaal. Elke stap gebruikt een andere willekeurige batch; sommige batches zijn lastiger dan andere, bijvoorbeeld met zeldzame tokens of “moeilijke” contexten. Daardoor kan de batch‑loss kort omhoog schieten. Zolang de <em>algemene trend</em> dalend is, leert het model. Grote, plotselinge pieken kunnen ook ontstaan als de learning rate hoog is of wanneer het model een lastig deel van de data tegenkomt.',
      numbersAboveGraph: 'Getallen boven de grafiek',
      numbersAboveGraphBody: '<strong>Batch‑loss</strong> is de waarde voor de huidige stap (wat je op de lijn ziet). <strong>Train‑loss</strong> en <strong>Dev‑loss</strong> zijn gemiddelden over de volledige train‑ en dev‑set en worden elke “Eval elke”-stappen bijgewerkt. Ze zijn gladder en laten zien of het model echt generaliseert (dev‑loss daalt) of overfit (train daalt, dev stijgt).',
      lowerLossNote: 'Lagere loss = betere voorspellingen. Het doel is dat de lijn daalt en dat de dev‑loss dicht bij of onder de train‑loss blijft.',
    },
    transformer: {
      title: 'Hoe het model één stap ziet',
      description: 'Een <strong class="text-neon">transformer</strong> is een neuraal netwerk dat tekst verwerkt, bijvoorbeeld de letters in een naam, door elk teken (1e, 2e, 3e, enzovoort) naar alle andere te laten &ldquo;kijken&rdquo;. Anders dan gewone feed-forward netwerken, die één teken tegelijk verwerken, of recurrent netwerken (RNN&rsquo;s), die stap voor stap werken en moeite hebben met lange tekst, zien transformers alle tekens tegelijk. Voeg <strong>attention</strong> toe, waarbij elk teken naar relevante andere kan &ldquo;kijken&rdquo;, en je krijgt een model dat goed werkt voor taal. Bij een naam als &ldquo;anna&rdquo; leest het model niet letter voor letter. Het ziet de hele string en laat elk teken de interpretatie van de rest beïnvloeden.',
      intro: 'De data stroomt <strong class="text-neon">van boven naar beneden</strong>. We beginnen met “welk teken?” en “waar in de naam?”, zetten die om in vectoren, combineren en normaliseren, en voeren dat door het transformerblok (attention + kleine MLP). Uiteindelijk krijgen we scores voor het volgende teken.',
      diagramHint: 'Sleep om te pannen, scroll om in te zoomen.',
      oneForwardPassNote: 'Dit is één “forward pass” voor één teken; dezelfde structuur herhaalt zich voor elk teken dat het model voorspelt.',
    },
    generatedName: {
      title: 'Hoe de Genereer-knop werkt',
      whatItDoes: 'Wat hij doet',
      whatItDoesBody: 'De Genereer-knop laat het getrainde model een nieuwe naam of woord produceren. Het model krijgt steeds één teken per stap en voorspelt het volgende teken totdat het stopt (of de maximale lengte bereikt). De uitvoer die je ziet wordt live gegenereerd met de huidige modelgewichten.',
      howItWorks: 'Hoe de uitvoer tot stand komt',
      howItWorksBody: 'Het model start met een speciaal begintoken. Bij elke stap: (1) voert het een forward pass door de transformer uit om scores voor elk teken in het vocabulaire te krijgen, (2) die scores worden met softmax en temperatuur 0,5 omgezet in kansen, en (3) er wordt willekeurig één teken uit die verdeling gesamplet. Dat teken wordt aan de reeks toegevoegd en als invoer voor de volgende stap gebruikt. Het stopt wanneer het model een eindtoken voorspelt of na 42 tekens. Lagere temperatuur betekent voorspelbaardere uitvoer; hoger zou meer variatie (en soms vreemdere namen) geven.',
      lastGeneratedLabel: 'Laatste gegenereerde uitvoer',
      lastGeneratedEmpty: '—',
    },
  },
  samplePlaceholder: '...',
  dataStatsTemplate: 'woorden={words} | vocab={vocab} | train/dev/test={train}/{dev}/{test}',
};

