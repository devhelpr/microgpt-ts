import type { LocaleStrings } from './types';

export const en: LocaleStrings = {
  errors: {
    appRootNotFound: 'App root not found',
    missingUiElement: 'Missing required UI element',
    diagramRenderFailed: 'Diagram could not be drawn. Try refreshing.',
  },
  defaultDataset: ['anna', 'bob', 'carla', 'diana', 'elias', 'frank', 'lucas', 'mila', 'nora'].join('\n'),
  flowStages: [
    { id: 'dataset', title: '1. Dataset', description: 'Read names and split into train/dev/test.' },
    { id: 'encode', title: '2. Encoding', description: 'Map characters to integer token IDs (BOS = end-of-sequence).' },
    { id: 'context', title: '3. Context Window', description: 'Fixed block_size context; each position predicts the next token.' },
    { id: 'forward', title: '4. Forward Pass', description: 'Token + position embedding → RMSNorm → Attention (Q,K,V, multi-head) → residual → RMSNorm → MLP (ReLU) → residual → lm_head → logits.' },
    { id: 'softmax', title: '5. Softmax', description: 'Convert logits to probabilities over next characters.' },
    { id: 'loss', title: '6. Loss', description: 'Cross-entropy compares predicted distribution vs target token.' },
    { id: 'backprop', title: '7. Backprop', description: 'Autograd computes gradients for each parameter.' },
    { id: 'update', title: '8. Update', description: 'Adam: m = β1·m + (1−β1)·g, v = β2·v + (1−β2)·g²; param -= lr·m_hat/(√v_hat + ε).' },
  ],
  illustrationLabels: {
    dataset: { train: 'train', dev: 'dev', test: 'test', names: 'names', namesPerLine: '1 per line' },
    encode: { charA: 'a', charN: 'n', id0: '0', id1: '1' },
    context: { next: 'next?', pos0: 'pos 0', block: 'block' },
    forward: { embed: '+ embed', attn: 'Attn', mlp: 'MLP', logits: 'logits' },
    softmax: { logits: 'logits', raw: '(raw)', expSum: 'exp / Σ', probsSum1: 'probs Σ = 1' },
    loss: { pTarget: 'p(target)', probability: 'probability', negLog: '−log(·)', L: 'L', loss: 'loss' },
    backprop: { params: 'params', gradLabel: '∂L∕∂', L: 'L', backward: 'backward' },
    update: { param: 'param', adam: 'Adam', adamFormula: 'm,v ← grad · param −= lr·m̂/√v̂', updated: 'updated' },
  },
  explainerBodies: {
    dataset: `We start with a list of names (one per line). That list is split into three parts: <strong>train</strong>, <strong>dev</strong>, and <strong>test</strong>. The model learns only from the train set. The dev set tells us how well it's doing while we train (e.g. the "dev loss" you see). The test set is kept aside until the end to measure final performance. Usually we use about 80% for training, 10% for dev, and 10% for test.`,
    encode: `The model can't work with letters directly; it needs numbers. So every character (like <code>a</code>, <code>n</code>) is turned into a number called a <strong>token ID</strong>. A special token marks the start of a name (<strong>BOS</strong> = beginning of sequence). Later, an "embedding" layer turns these IDs into vectors of numbers the model actually uses. Think of it as: letters → ID numbers → rich number vectors.<br/><br/><strong>What are tokens in an LLM context?</strong> A <strong>token</strong> is the smallest piece of text the model works with: here, one character; in bigger models it might be a subword or word. The <strong>context</strong> is the list of tokens we give the model at once (e.g. the previous few characters). The model reads that context and predicts the next token. So "context" = the input sequence; "token" = one item in that sequence (or the one we're predicting).`,
    context: `The model doesn't see the whole name at once. It only looks at a fixed number of previous characters; that's the <strong>context window</strong> (or block). At each step it tries to guess the <strong>next</strong> character. For example, if the context is <code>a, n, n</code>, the target might be <code>a</code> (for "anna"). Then we slide by one character and repeat. So the same name produces many small "predict the next character" tasks.`,
    forward: `This is one full pass through the network. We take the token IDs and their positions, turn them into vectors (embeddings), and add them. Then we run that through: a normalization step (<strong>RMSNorm</strong>), <strong>attention</strong> (so each position can look at the others), a small "MLP" block, and finally a head that outputs one score per possible next character; those scores are the <strong>logits</strong>. No learning happens here; we're just computing the model's current prediction.`,
    softmax: `The network outputs raw scores (logits). We need probabilities: "how likely is each character to come next?" <strong>Softmax</strong> does that. It turns the logits into numbers between 0 and 1 that add up to 1 (like a proper probability distribution). The formula is: each probability = exp(logit) divided by the sum of exp of all logits. Training tries to make the probability of the correct next character as high as possible.`,
    loss: `We need one number that says "how wrong was the prediction?" That's the <strong>loss</strong>. Here we use <strong>cross-entropy</strong>: <code>L = -log(probability we gave to the correct character)</code>. If the model was confident and right, that probability is high and the loss is low. If it was wrong or unsure, the loss is higher. Training aims to make this loss smaller over time.`,
    backprop: `After we have the loss, we need to know how to change every weight in the network to reduce it. <strong>Backpropagation</strong> does that: it works backward from the loss through every layer (attention, MLP, embeddings) and computes a <strong>gradient</strong> for each parameter. The gradient tells us the direction and (roughly) how much to adjust. The "gradient norm" you see is a single number summarizing how big those gradients are.<br/><br/><strong>What does ∂L∕∂ mean?</strong> The symbol <strong>∂L∕∂</strong> (read "partial d L over partial d …") is calculus notation for <em>how much the loss L changes when you change one parameter</em>. For each weight we get a number: <code>∂L∕∂(weight)</code>. If that number is positive, increasing the weight would increase the loss (so we decrease the weight). If it's negative, we increase the weight. So ∂L∕∂ is exactly the gradient: it tells us, for every parameter, which way to nudge it to reduce the loss.`,
    update: `We have gradients; now we actually change the weights. We use <strong>Adam</strong>, a popular optimizer. It keeps a little "memory" (momentum) and "spread" (variance) per parameter, then updates each weight using the learning rate and those terms: <code>param -= lr · m_hat / (√v_hat + ε)</code>. The learning rate often gets smaller over training (e.g. linear decay), so steps are smaller near the end.`,
  },
  aria: {
    close: 'Close',
    learnMoreAbout: 'Learn more about',
    explainTrainingDynamics: 'Explain training dynamics graph',
  },
  header: {
    panelTitle: 'microgpt in browser',
    title: 'Visualize the full algorithm, live',
    description: 'This dashboard explains each phase of microGPT and updates it with real values while training: context tokens, probabilities, loss, gradient norm, and SGD updates.',
  },
  generatedName: {
    panelTitle: 'Generated name',
    generate: 'Generate',
  },
  controls: {
    panelTitle: 'Controls',
    datasetLabel: 'Dataset (1 name per line)',
    maxSteps: 'Max steps',
    evalEvery: 'Eval every',
    start: 'Start',
    pause: 'Pause',
    reset: 'Reset',
    tipReset: 'Tip: reset after changing dataset/hyperparameters.',
  },
  trainingDynamics: {
    panelTitle: 'Training Dynamics',
    explainBtn: 'Explain training dynamics graph',
    statusIdle: 'idle',
    statusTraining: 'training',
    statusCompleted: 'completed',
    step: 'Step',
    batchLoss: 'Batch Loss',
    trainLoss: 'Train Loss',
    devLoss: 'Dev Loss',
  },
  algorithm: {
    panelTitle: 'How The Algorithm Works',
    review: 'Review:',
    live: 'Live',
    viewTransformerDiagram: 'View transformer diagram',
  },
  breakdown: {
    panelTitle: 'Current Step Breakdown',
    stepBreakdown: 'Step {n} Breakdown',
    contextIds: 'Context IDs:',
    contextTokens: 'Context tokens:',
    target: 'Target:',
    predicted: 'Predicted:',
    learningRate: 'Learning rate:',
    gradientNorm: 'Gradient norm:',
  },
  probabilities: {
    panelTitle: 'Probabilities (Current Step)',
  },
  languages: {
    label: 'Language',
    en: 'English',
    nl: 'Dutch',
  },
  iterationSelect: {
    stepLabel: 'Step {n}',
    trainDevLabel: 'train={trainLoss} dev={devLoss}',
  },
  chart: {
    min: 'min',
    max: 'max',
  },
  vectorBars: {
    first8Dims: '(first 8 dims)',
  },
  stageVisual: {
    dataset: {
      splitDescription: 'Split of docs (names) used for train/dev/test.',
      train: 'train',
      dev: 'dev',
      test: 'test',
    },
    encode: {
      description: 'Characters become token IDs before training.',
      contextTokensToIds: 'context tokens -> ids',
      target: 'target:',
    },
    context: {
      description: 'Sliding context window predicts next token.',
    },
    forward: {
      attentionHead0: 'Attention (head 0) over context positions',
      tokenEmbedding: 'token embedding',
      positionEmbedding: 'position embedding',
      sumEmbedding: 'sum embedding',
      preHeadAfterMlp: 'pre-head (after RMSNorm + MLP block)',
      preHeadBeforeLmHead: 'pre-head (before lm_head)',
    },
    softmax: {
      description: 'Logits converted into normalized probabilities.',
    },
    loss: {
      description: 'Cross-entropy for the true next token.',
      predictedTarget: 'Predicted {pred}, target {target}',
    },
    backprop: {
      description: 'Backward pass computes gradients through the graph.',
      gradientNorm: 'gradient norm',
    },
    update: {
      description: 'Adam: m = β1·m + (1−β1)·g, v = β2·v + (1−β2)·g²; param -= lr·m_hat/(√v_hat + ε).',
      formula: 'param -= lr * m_hat / (sqrt(v_hat) + ε)',
      lrStepMagnitude: 'lr={lr} | avg step magnitude≈{delta}',
    },
  },
  mermaid: {
    whichCharacter: 'Which character?',
    whichPosition: 'Which position?',
    turnCharIntoVector: 'Turn character into a vector',
    addPositionAsVector: 'Add position as a vector',
    combineBoth: 'Combine both',
    stabilizeScale: 'Stabilize scale',
    transformerBlock: 'Transformer block × {n}',
    stabilize: 'Stabilize',
    attentionMixContext: 'Attention: mix with context',
    addShortcut: 'Add shortcut',
    smallFeedForward: 'Small feed-forward',
    predictNextChar: 'Predict next character',
    scoresForEachChar: 'Scores for each character',
  },
  dialogs: {
    trainingDynamics: {
      title: 'Understanding the Training Dynamics Graph',
      whatGraphShows: 'What this graph shows',
      whatGraphShowsBody: 'The line plots <strong>batch loss</strong> over training steps (time runs left to right). Each point is the loss on one mini-batch: how wrong the model was on that small chunk of data. The chart shows the last 300 steps so you can see recent trends.',
      spikesMean: 'What do the spikes mean?',
      spikesMeanBody: 'Spikes are normal. Every step uses a different random batch, so some batches are harder than others; the model might see rare tokens or tricky contexts. That makes the batch loss jump up briefly. As long as the <em>overall trend</em> is downward, the model is learning. Big, sudden spikes can also happen when the learning rate is high or when the model hits a difficult part of the data.',
      numbersAboveGraph: 'Numbers above the graph',
      numbersAboveGraphBody: '<strong>Batch loss</strong> is the value for the current step (what you see on the line). <strong>Train loss</strong> and <strong>Dev loss</strong> are averaged over the full train and dev sets and are updated every "Eval every" steps. They\'re smoother and tell you whether the model is actually generalizing (dev loss going down) or overfitting (train down, dev up).',
      lowerLossNote: 'Lower loss = better predictions. The goal is for the line to trend down and for dev loss to stay close to or below train loss.',
    },
    transformer: {
      title: 'How the model sees one step',
      intro: 'Data flows <strong class="text-neon">top to bottom</strong>. We start with &ldquo;which character?&rdquo; and &ldquo;where in the sequence?&rdquo;, turn them into vectors, combine and normalize, then run the transformer block (attention + small MLP). Finally we get scores for the next character.',
      oneForwardPassNote: 'This is one &ldquo;forward pass&rdquo; for a single position; the same structure repeats for each character the model predicts.',
    },
  },
  samplePlaceholder: '...',
  dataStatsTemplate: 'words={words} | vocab={vocab} | train/dev/test={train}/{dev}/{test}',
};
