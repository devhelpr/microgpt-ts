import type { LocaleStrings } from './types';

export const en: LocaleStrings = {
  errors: {
    appRootNotFound: 'App root not found',
    missingUiElement: 'Missing required UI element',
    diagramRenderFailed: 'Diagram could not be drawn. Try refreshing.',
  },
  defaultDataset: ['anna', 'bob', 'carla', 'diana', 'elias', 'frank', 'lucas', 'mila', 'nora'].join('\n'),
  flowStages: [
    { id: 'dataset', title: '1. Dataset', description: 'Read names and split into train/dev/test.' },
    { id: 'encode', title: '2. Encoding', description: 'Map characters to integer token IDs (BOS = end-of-sequence).' },
    { id: 'context', title: '3. Context Window', description: 'Fixed block_size context; each position predicts the next token.' },
    { id: 'forward', title: '4. Forward Pass', description: 'Token + position embedding → RMSNorm → Attention (Q,K,V, multi-head) → residual → RMSNorm → MLP (ReLU) → residual → lm_head → logits.' },
    { id: 'softmax', title: '5. Softmax', description: 'Convert logits to probabilities over next characters.' },
    { id: 'loss', title: '6. Loss', description: 'Cross-entropy compares predicted distribution vs target token.' },
    { id: 'backprop', title: '7. Backprop', description: 'Autograd computes gradients for each parameter.' },
    { id: 'update', title: '8. Update', description: 'Adam: m = β1·m + (1−β1)·g, v = β2·v + (1−β2)·g²; param -= lr·m_hat/(√v_hat + ε).' },
  ],
  illustrationLabels: {
    dataset: { train: 'train', dev: 'dev', test: 'test', names: 'names', namesPerLine: '1 per line' },
    encode: { charA: 'a', charN: 'n', id0: '0', id1: '1' },
    context: { next: 'next?', pos0: 'pos 0', block: 'block' },
    forward: { embed: '+ embed', attn: 'Attn', mlp: 'MLP', logits: 'logits' },
    softmax: { logits: 'logits', raw: '(raw)', expSum: 'exp / Σ', probsSum1: 'probs Σ = 1' },
    loss: { pTarget: 'p(target)', probability: 'probability', negLog: '−log(·)', L: 'L', loss: 'loss' },
    backprop: { params: 'params', gradLabel: '∂L∕∂', L: 'L', backward: 'backward' },
    update: { param: 'param', adam: 'Adam', adamFormula: 'm,v ← grad · param −= lr·m̂/√v̂', updated: 'updated' },
  },
  explainerBodies: {
    dataset: `We start with a list of names (one per line). That list is split into three parts: <strong>train</strong>, <strong>dev</strong>, and <strong>test</strong>. The model learns only from the train set. The dev set tells us how well it's doing while we train (e.g. the "dev loss" you see). The test set is kept aside until the end to measure final performance. Usually we use about 80% for training, 10% for dev, and 10% for test.`,
    encode: `The model can't work with letters directly; it needs numbers. So every character (like <code>a</code>, <code>n</code>) is turned into a number called a <strong>token ID</strong>. A special token marks the start of a name (<strong>BOS</strong> = beginning of sequence). Later, an "embedding" layer turns these IDs into vectors of numbers the model actually uses. Think of it as: letters → ID numbers → rich number vectors.<br/><br/><strong>What are tokens in an LLM context?</strong> A <strong>token</strong> is the smallest piece of text the model works with: here, one character; in bigger models it might be a subword or word. The <strong>context</strong> is the list of tokens we give the model at once (e.g. the previous few characters). The model reads that context and predicts the next token. So "context" = the input sequence; "token" = one item in that sequence (or the one we're predicting).`,
    context: `The model doesn't see the whole name at once. It only looks at a fixed number of previous characters; that's the <strong>context window</strong> (or block). At each step it tries to guess the <strong>next</strong> character. For example, if the context is <code>a, n, n</code>, the target might be <code>a</code> (for "anna"). Then we slide by one character and repeat. So the same name produces many small "predict the next character" tasks.`,
    forward: `This is one full pass through the network. We take the token IDs and their positions, turn them into vectors (embeddings), and add them. Then we run that through: a normalization step (<strong>RMSNorm</strong>), <strong>attention</strong> (so each position can look at the others), a small "MLP" block, and finally a head that outputs one score per possible next character; those scores are the <strong>logits</strong>. No learning happens here; we're just computing the model's current prediction.`,
    softmax: `The network outputs raw scores (logits). We need probabilities: "how likely is each character to come next?" <strong>Softmax</strong> does that. It turns the logits into numbers between 0 and 1 that add up to 1 (like a proper probability distribution). The formula is: each probability = exp(logit) divided by the sum of exp of all logits. Training tries to make the probability of the correct next character as high as possible.`,
    loss: `We need one number that says "how wrong was the prediction?" That's the <strong>loss</strong>. Here we use <strong>cross-entropy</strong>: <code>L = -log(probability we gave to the correct character)</code>. If the model was confident and right, that probability is high and the loss is low. If it was wrong or unsure, the loss is higher. Training aims to make this loss smaller over time.`,
    backprop: `After we have the loss, we need to know how to change every weight in the network to reduce it. <strong>Backpropagation</strong> does that: it works backward from the loss through every layer (attention, MLP, embeddings) and computes a <strong>gradient</strong> for each parameter. The gradient tells us the direction and (roughly) how much to adjust. The "gradient norm" you see is a single number summarizing how big those gradients are.<br/><br/><strong>What does ∂L∕∂ mean?</strong> The symbol <strong>∂L∕∂</strong> (read "partial d L over partial d …") is calculus notation for <em>how much the loss L changes when you change one parameter</em>. For each weight we get a number: <code>∂L∕∂(weight)</code>. If that number is positive, increasing the weight would increase the loss (so we decrease the weight). If it's negative, we increase the weight. So ∂L∕∂ is exactly the gradient: it tells us, for every parameter, which way to nudge it to reduce the loss.`,
    update: `We have gradients; now we actually change the weights. We use <strong>Adam</strong>, a popular optimizer. It keeps a little "memory" (momentum) and "spread" (variance) per parameter, then updates each weight using the learning rate and those terms: <code>param -= lr · m_hat / (√v_hat + ε)</code>. The learning rate often gets smaller over training (e.g. linear decay), so steps are smaller near the end.`,
  },
  aria: {
    close: 'Close',
    learnMoreAbout: 'Learn more about',
    explainTrainingDynamics: 'Explain training dynamics graph',
  },
  header: {
    panelTitle: 'microgpt in browser',
    title: 'Visualize the full algorithm, live',
    subtitle: 'TypeScript port of <a href="https://karpathy.github.io/2026/02/12/microgpt/" target="_blank" rel="noopener noreferrer" class="underline text-neon/90 hover:text-neon focus:outline focus:ring-2 focus:ring-neon/50 rounded">microgpt</a> by <a href="https://karpathy.github.io/" target="_blank" rel="noopener noreferrer" class="underline text-neon/90 hover:text-neon focus:outline focus:ring-2 focus:ring-neon/50 rounded">Andrej Karpathy</a>.',
    description: 'This dashboard explains each phase of microGPT and updates it with real values while training: context tokens, probabilities, loss, gradient norm, and parameter updates.',
  },
  generatedName: {
    panelTitle: 'Generated name',
    generate: 'Generate',
    infoBtn: 'Explain what the Generate button does',
  },
  controls: {
    panelTitle: 'Controls',
    datasetLabel: 'Dataset (1 name per line)',
    maxSteps: 'Max steps',
    evalEvery: 'Eval every',
    start: 'Start',
    pause: 'Pause',
    reset: 'Reset',
    tipReset: 'Tip: reset after changing dataset/hyperparameters.',
  },
  trainingDynamics: {
    panelTitle: 'Training Dynamics',
    explainBtn: 'Explain training dynamics graph',
    statusIdle: 'idle',
    statusTraining: 'training',
    statusCompleted: 'completed',
    step: 'Step',
    batchLoss: 'Batch Loss',
    trainLoss: 'Train Loss',
    devLoss: 'Dev Loss',
  },
  algorithm: {
    panelTitle: 'How The Algorithm Works',
    review: 'Review:',
    live: 'Live',
    viewTransformerDiagram: 'Explain the transformer structure and flow',
  },
  breakdown: {
    panelTitle: 'Current Step Breakdown',
    stepBreakdown: 'Step {n} Breakdown',
    contextText: 'Context text:',
    noContextText: '(no real context yet, only the internal start token)',
    contextIds: 'Context IDs:',
    contextTokens: 'Context tokens:',
    contextTokensBosHint: '(showing only characters from your data; internal markers are hidden)',
    target: 'Target:',
    predicted: 'Predicted:',
    learningRate: 'Learning rate:',
    gradientNorm: 'Gradient norm:',
  },
  probabilities: {
    panelTitle: 'Probabilities (Current Step)',
    explainer:
      'Each row is a possible next token. The bar length and the number (0–1) show the model’s predicted probability for that token.',
  },
  languages: {
    label: 'Language',
    en: 'English',
    nl: 'Dutch',
  },
  iterationSelect: {
    stepLabel: 'Step {n}',
    trainDevLabel: 'train={trainLoss} dev={devLoss}',
  },
  chart: {
    min: 'min',
    max: 'max',
  },
  vectorBars: {
    first8Dims: '(first 8 dims)',
  },
  stageVisual: {
    dataset: {
      splitDescription: 'Split of docs (names) used for train/dev/test.',
      train: 'train',
      dev: 'dev',
      test: 'test',
    },
    encode: {
      description: 'Characters become token IDs before training.',
      contextTokensToIds: 'context tokens -> ids',
      target: 'target:',
    },
    context: {
      description: 'Sliding context window predicts next token.',
    },
    forward: {
      attentionHead0: 'Attention (head 0) over context positions',
      tokenEmbedding: 'token embedding',
      positionEmbedding: 'position embedding',
      sumEmbedding: 'sum embedding',
      preHeadAfterMlp: 'pre-head (after RMSNorm + MLP block)',
      preHeadBeforeLmHead: 'pre-head (before lm_head)',
    },
    softmax: {
      description: 'Logits converted into normalized probabilities.',
    },
    loss: {
      description: 'Cross-entropy for the true next token.',
      predictedTarget: 'Predicted {pred}, target {target}',
    },
    backprop: {
      description: 'Backward pass computes gradients through the graph.',
      gradientNorm: 'gradient norm',
    },
    update: {
      description: 'Adam: m = β1·m + (1−β1)·g, v = β2·v + (1−β2)·g²; param -= lr·m_hat/(√v_hat + ε).',
      formula: 'param -= lr * m_hat / (sqrt(v_hat) + ε)',
      lrStepMagnitude: 'lr={lr} | avg step magnitude≈{delta}',
    },
  },
  mermaid: {
    whichCharacter: 'Which character?',
    whichPosition: 'Which position?',
    turnCharIntoVector: 'Turn character into a vector',
    addPositionAsVector: 'Add position as a vector',
    combineBoth: 'Combine both',
    stabilizeScale: 'Stabilize scale',
    transformerBlock: 'Transformer block × {n}',
    stabilize: 'Stabilize',
    attentionMixContext: 'Attention: mix with context',
    addShortcut: 'Add shortcut',
    smallFeedForward: 'Small feed-forward',
    predictNextChar: 'Predict next character',
    scoresForEachChar: 'Scores for each character',
  },
  transformerDiagramExplainers: {
    A: 'This step answers: which character is at this position in the input? For example, if the model is predicting after "ann", the character at position 3 might be "a". The model needs to know the raw input character so it can convert it into a numerical representation.',
    B: 'Each character (letter, digit, or symbol) is turned into a vector of numbers called an embedding. Similar characters get similar vectors. This mapping is learned during training and lets the model work with numbers instead of raw text.',
    C: 'This step answers: where in the sequence is this character? For "anna", the first "a" is at position 0, "n" at 1, another "n" at 2, and the last "a" at 3. Position matters because the same character can mean different things depending on where it appears.',
    D: 'The position index is encoded as a vector (using sine and cosine functions) and added to the character embedding. This gives the model a sense of order: it can tell the first "a" from the last "a" in "anna".',
    E: 'The character embedding and the position embedding are added together. The result is a single vector that encodes both what the character is and where it sits in the sequence.',
    F: 'LayerNorm scales and shifts the values so they stay in a healthy range. This makes training more stable and helps the model learn faster. Without it, numbers can grow or shrink uncontrollably through the layers.',
    TB: 'This is the core of the transformer. Inside, self-attention lets each position "look at" all other positions to gather context. Then a small feed-forward network processes each position. The block can be repeated multiple times (here, once) to build up richer representations.',
    G1: 'LayerNorm is applied before the attention step. It normalizes the incoming vectors so the attention mechanism receives values in a consistent scale.',
    G2: 'Self-attention is the key innovation: each position can attend to every other position. For example, when predicting the next character, the model can look at all previous characters and decide which ones are most relevant. This allows long-range dependencies.',
    G3: 'A residual (or shortcut) connection adds the original input back to the output of attention. This helps gradients flow during training and makes it easier to learn; the model can "skip" the attention layer if needed.',
    G4: 'LayerNorm is applied again before the feed-forward step. Each sublayer (attention, then feed-forward) is preceded by normalization.',
    G5: 'A small feed-forward network: two linear layers with a non-linear activation (like ReLU) in between. It processes each position independently, allowing the model to transform the representations further.',
    G6: 'Another residual connection adds the input of the feed-forward block back to its output. Combined with G3, these shortcuts help the model learn effectively.',
    H: 'A final linear layer maps the transformed vector to a score for each character in the vocabulary. The output has one number per possible character.',
    I: 'These scores indicate how likely each character is to come next. The model picks the character with the highest score as its prediction. In practice, you might sample from a distribution instead of always taking the top one.',
  },
  dialogs: {
    trainingDynamics: {
      title: 'Understanding the Training Dynamics Graph',
      whatGraphShows: 'What this graph shows',
      whatGraphShowsBody: 'The line plots <strong>batch loss</strong> over training steps (time runs left to right). Each point is the loss on one mini-batch: how wrong the model was on that small chunk of data. The chart shows the last 300 steps so you can see recent trends.',
      spikesMean: 'What do the spikes mean?',
      spikesMeanBody: 'Spikes are normal. Every step uses a different random batch, so some batches are harder than others; the model might see rare tokens or tricky contexts. That makes the batch loss jump up briefly. As long as the <em>overall trend</em> is downward, the model is learning. Big, sudden spikes can also happen when the learning rate is high or when the model hits a difficult part of the data.',
      numbersAboveGraph: 'Numbers above the graph',
      numbersAboveGraphBody: '<strong>Batch loss</strong> is the value for the current step (what you see on the line). <strong>Train loss</strong> and <strong>Dev loss</strong> are averaged over the full train and dev sets and are updated every "Eval every" steps. They\'re smoother and tell you whether the model is actually generalizing (dev loss going down) or overfitting (train down, dev up).',
      lowerLossNote: 'Lower loss = better predictions. The goal is for the line to trend down and for dev loss to stay close to or below train loss.',
    },
    transformer: {
      title: 'How the model sees one step',
      description: 'A <strong class="text-neon">transformer</strong> is a neural network that processes text, such as the letters in a name, by letting every character slot (1st, 2nd, 3rd, and so on) attend to every other. Unlike regular feed-forward networks, which handle one character at a time, or recurrent networks (RNNs), which process step by step and struggle with long text, transformers see all characters at once. Add <strong>attention</strong>, where each character can &ldquo;look at&rdquo; relevant others, and you get a model that works well for language. For a name like &ldquo;anna&rdquo;, the model doesn&rsquo;t read letter by letter. It sees the whole string and lets each letter influence how it interprets the rest.',
      intro: 'Data flows <strong class="text-neon">top to bottom</strong>. We start with &ldquo;which character?&rdquo; and &ldquo;where in the name?&rdquo;, turn them into vectors, combine and normalize, then run the transformer block (attention + small MLP). Finally we get scores for the next character.',
      diagramHint: 'Drag to pan, scroll to zoom.',
      oneForwardPassNote: 'This is one &ldquo;forward pass&rdquo; for a single character slot; the same structure repeats for each character the model predicts.',
    },
    generatedName: {
      title: 'How the Generate Button Works',
      whatItDoes: 'What it does',
      whatItDoesBody: 'The Generate button runs your trained model to produce a new name or word. It feeds the model one character at a time, and the model predicts the next character until it stops (or reaches the maximum length). The output you see is generated live using the current model weights.',
      howItWorks: 'How it produces the output',
      howItWorksBody: 'The model starts from a special beginning token, then at each step: (1) it runs a forward pass through the transformer to get scores for every character in the vocabulary, (2) those scores are turned into probabilities using softmax with temperature 0.5, and (3) it randomly samples one character from that distribution. That character is appended to the sequence and fed as input for the next step. It stops when the model predicts an end token or after 42 characters. Lower temperature means more deterministic output; higher would mean more varied (and sometimes weirder) names.',
      lastGeneratedLabel: 'Last generated output',
      lastGeneratedEmpty: '—',
    },
  },
  samplePlaceholder: '...',
  dataStatsTemplate: 'words={words} | vocab={vocab} | train/dev/test={train}/{dev}/{test}',
};
